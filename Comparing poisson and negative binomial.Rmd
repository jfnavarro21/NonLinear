---
title: "NL Week 5 Workshop 2"
author: "John Navarro"
date: "February 2, 2018"
output: pdf_document
---

# 1. Example from slide 8 in the Lecture Notes

Web server is currently registering a Poisson flow of visitors with average rate 15 visits per hour.

What is the probability of receiving at least 10 visits in the next 30 minutes?

```{r}
# ppois() gives the probability of x or less occuring in a given period
# the lambda is 15/hour, so for next 30 min, we use lambda = 7.5
# we use 1-ppois to get the probability above the threshold of 10 visits
1-ppois(9,7.5)
```
What is the probability of receiving more than 10 visits in the next hour?
```{r}
1-ppois(10,15)
```
# 2. Poisson Regression

Poisson regression is based on log link, i.e. log of intensity of Poisson process is the response of linear model:

ln(??) = ??0+??1x1+.+??pxp.

Then probability distribution of counts is given by: (dpois function)

P(Np=k;??)=(e^????? * ??^k)/k!,

where lambda is:

??= e^??i = e^(??0+??1x1+.+??pxp)

Change of X by 1 shows how much ln(lambda) changes



## 2.1 Data

Data set gala contains counts of plant species on each of 30 Galapagos Islands and the number that are endemic.
There are also 5 geographic variables for each island

```{r}
library(faraway)
library(MASS)
data(gala)
head(gala)
```
Remove the endemics column
```{r}
gala <- gala[,-2]
```
## 2.2 Linear Regression

Fit Linear regression and check the residuals

```{r}
modl <- lm(Species ~ ., data=gala)
plot(predict(modl), residuals(modl), xlab="Fitted", ylab="Residuals")
```

This plot shows clustering at low values, and increasing variance (heteroskedasticity)
Use the square root transformation to stabilize the variance
```{r}
modt <- lm(sqrt(Species) ~ ., data=gala)
plot(predict(modt), residuals(modt), xlab="Fitted", ylab="Residuals")

```
Now the points on the plot do not cluster as much
```{r}
summary(modt)
```
The fit looks pretty good, according to determination coefficient
```{r}
hist(modt$residuals)
qqnorm(modt$residuals)
qqline(modt$residuals)
```
The residuals do not appear to be normally distributed. The Gaussian assumption is in question. The estimates are biased and we have wide confidence intervals.
See if it is possible to keep the nature of the data untransformed(counts, rather than square root) and find a better explanation of the data
```{r}
hist(gala[,1])
```
```{r}
# Calculate the lambda
(poisParam <- fitdistr(gala[,1], "poisson"))
# lambda is significant
# We have to reject, it is not Poisson
ks.test(gala[,1], "ppois", poisParam$estimate)
```
This fit is not great for Poisson model, but continues

## 2.3 Poisson regression

Fit the model
```{r}
modp <- glm(Species ~ ., family = poisson, gala)
summary(modp)
```

The fitted model has the form
ln(??i) = ??i = ??0+??1x1,i+??2x2,i+??3x3,i+??4x4,i+??5x5,i,
where ??i=E[Yi].

Prediction of ??i is done by

```{r}
predict(modp)
```
```{r}
predict(modp, type="link")
```
Prediction of lambda, by takint the exponenet
```{r}
predict(modp, type="response")
```
```{r}
exp(predict(modp))
```
Finally, predicted probabilities of counts can be calculated as, for example for count zero:
```{r}
coun<-0
dpois(coun,predict(modp,type="response"))
```
We see that all predictors are significant.
However, residual deviance of 716.85 with 24 degrees of freedom do not suggest a good fit.

These data may not be described well by the Poisson regression.

There may be overdispersion.

## 2.4 Methods for Testing Overdispersion

There are several ways of testing for overdispersion

### 2.4.1 A quick and rough method

Look at the output of glm() and compare the residual deviance with the number of degrees of freedom.
If the assumed model is correct deviance is asymptotically distributed as Chi-squared (X2) with degrees of freedom n???k where n is the number of observations and k is the number of parameters.
For Chi-squared distribution the mean is the number of degrees of freedom n???k.
If the residual deviance returned by glm() is greater than n???k then it might be a sign of over-dispersion.

Test the method on simulated Poisson data.
```{r}
# create a function that checks for overdispersion
# arguments are sample size and lambda
Test.Deviance.Overdispersion.Poisson<-function(Sample.Size,Parameter.Lambda){
  # create a poisson distributed sample
  my.Sample<-rpois(Sample.Size,Parameter.Lambda)
  # fit poisson regression
  Model<-glm(my.Sample~1,family=poisson)
  # extract the deviance
  Dev<-Model$deviance
  # extract the degrees of freedom
  Deg.Fred<-Model$df.residual
  # check that the z-score is between the -1.96 and 1.96 thresholds
  ((Dev-Deg.Fred)/sqrt(2*Deg.Fred)>-1.96)&((Dev-Deg.Fred)/sqrt(2*Deg.Fred)<1.96)
} 
set.seed(7324)
Test.Deviance.Overdispersion.Poisson(100,1)
```
The function simulates a sample from Poisson distribution, estimates parameter ?? which is simultaneously the mean value and the variance, then it checks if Deviance???Deg.Freedom2???Deg.Freedom??? belongs to the interval (???1.96,1.96).
If yes, the result is TRUE. Otherwise it is FALSE.

Now repeat the call of the function 300 times to see how many times it returns TRUE and how many times FALSE.
```{r}
sum(replicate(300,Test.Deviance.Overdispersion.Poisson(100,1)))
```
Note that the number of passes is lower than expected for 95% confidence interval.
This means that the test has a tendency to detect overdispersion when it is not present.

The estimate of the parameter ?? given by glm() is e^Coefficient:
```{r}
exp(glm(rpois(1000,2)~1,family=poisson)$coeff)
```
Perform the same test on negative binomial data (we know there will be overdispersion there)..

```{r}
Test.Deviance.Overdispersion.NBinom<-function(Sample.Size,Parameter.prob){
  my.Sample<-rnbinom(Sample.Size,2,Parameter.prob)
  Model<-glm(my.Sample~1,family=poisson)
  Dev<-Model$deviance
  Deg.Fred<-Model$df.residual
  ((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)>-1.96)&((Dev/Deg.Fred-1)/sqrt(2/Deg.Fred)<=1.96)
} 
sum(replicate(300,Test.Deviance.Overdispersion.NBinom(100,.2)))
```
We see that the over-dispersed negative binomial distribution sample rarely or never passes the test.

### 2.4.2 REgression test by Cameron-Trivedi

The test implemented in AER is described in Cameron, A.C. and Trivedi, P.K. (1990). Regression-based Tests for Over-dispersion in the Poisson Model. Journal of Econometrics, 46, 347-364.

In a Poisson model, the mean is E(Y)=?? and the variance is V(Y)=?? as well.
The test has a null hypothesis c=0 where Var(Y)=??+c???f(??), c<0 means under-dispersion and c>0 means over-dispersion.
The function f(.) is some monotonic function (linear as default or quadratic).
The test statistic used is a t statistic which is asymptotically standard normal under the null

```{r}
library(AER)
```
Learn how to use dispersiontest() from AER and apply it to GLM.model that we fit earlier
```{r}
dispersiontest(glm(rpois(100,1)~1, family="poisson"), alternative = "two.sided")
# cannot reject, there is no overdispersion
```
```{r}
# twosided either greater or less, not eqal to 0
dispersiontest(glm(rnbinom(100,size=20,prob=.7)~1,family="poisson"),alternative="two.sided")
```

In the first case the hypothesis H0: "dispersion=1" cannot be rejected.
In the second case it needs to be rejected with 5% level.

### 2.4.3 Test against Negative Binomial Distribution

The null hypothesis of this test is that the distribution is Poisson as particular case of Negative binomial against Negative Binomial.

The references are:
A. Colin Cameron and Pravin K. Trivedi (1998) Regression analysis of count data. New York: Cambridge University Press.

Lawless, J. F. (1987) Negative Binomial and Mixed Poisson Regressions. The Canadian Journal of Statistics. 15:209-225.

Required packages are MASS (to create a negative binomial object with glm.nb) and pscl, which contains the test function odTest().

```{r}
suppressWarnings(library(pscl))
```
Apply glm.nb() from MASS to fit a negative binomial model.
Then use odTest() from pscl to test if the data can be described by Poisson distribution (no over-dispersion) or not (over-dispersion).
```{r}
set.seed(958)
NB.model.pois<-suppressWarnings(glm.nb(rpois(100,2)~1))
names(NB.model.pois)
```

```{r}
NB.model.pois
summary(NB.model.pois)
```
The negative binomial model is estimated with deviance 96.0937388 and degrees of freedom 99. The AIC value is 353.3762775.

Parameter Theta in the summary() shows the amount of overdispersion.
If ?? is the mean value of counts according to negative binomial distribution then variance is ??+??2??.
So, if ?? is very large then both mean and variance are equal to ??, like in Poisson distribution.
But if ?? is small then variance is greater than ?? which means overdispersion relative to the Poisson distribution.
See discussion for more information.

The estimated parameter of the model is:

```{r}
NB.model.pois$coefficients
```
To find the intensity of the simulated Poisson process take exponent of the linear predictor:
```{r}
unique(exp(predict(NB.model.pois)))
unique(predict(NB.model.pois,type="response"))
exp(NB.model.pois$coefficients)
```

Use the model to test for overdispersion.
```{r}
odTest.pois<-odTest(NB.model.pois)
```

With high p-value the null hypothesis cannot be rejected.

Repeat the same steps with simulated negative binomial distribution with the same mean value.
```{r}
set.seed(958)
NB.model<-glm.nb(rnbinom(100,size=2,prob=.5)~1)
NB.model
summary(NB.model)
exp(NB.model$coefficients)
odTest.pois<-odTest(NB.model)
```
This time the null hypothesis is rejected signaling overdispersion.



### 2.4.4 Prediction by negative binomial regression

```{r}
# return theta from the NB model fit
(theta <- NB.model$theta)
```
```{r}
count <- 0
mu <- unique(predict(NB.model, new.data=data.frame(0), type="response"))
(prob.coun.1 <- unique(dnbinom(count, mu=mu, size=theta)))
```
```{r}
p<-theta/(mu+theta)
(prob.coun.2<-dnbinom(count,p=p,size=theta))
```
Probabilities are constant vectors because this simulated model does not have any predictors, only intercept.

# 3.Examples of Poisson, Negative Binomial and zero augmented regressions

## 3.1 Wave Soldering

In this example AT&T experimented with 5 parameters of wave soldering process for mounting components on printed circuit boards.

```{r}
library(faraway)
library(AER)
library(MASS)
library(pscl)
```
load the data
```{r}
data(solder)
head(solder)
```
The output variable skips is the count of solder skips.

The number of elements mounted by soldering is large enough to assume Poisson regression.

### 3.1.1 Poisson regression

Fit Poisson regression

```{r}
# fit a possion model
modp <- glm(skips~., family=poisson,data=solder)
summary(modp)
# return the deviance
(Dev <- deviance(modp))
# return the degrees of freedom
(Deg.Fred <- df.residual(modp))
# return the AIC
(aicComplete <- modp$aic)
```
Deviance of 1829 on 882 degrees of freedom does not look like a good fit.
Check for overdispersion using the 3 methods to confirm that.

Method 1 - Build a confindence interval of deviance and degrees of freedom.
Deviance is asymptotically Chi square distributed, defined by the deg of freedom.
If the Deviance is close to Expected value(d.f.), then we expect it to be a Poisson Distribution. 
```{r}
method1 <- c((Dev-Deg.Fred)/sqrt(2*Deg.Fred)-1.96, (Dev-Deg.Fred)/sqrt(2*Deg.Fred)+1.96)
print("Method 1")
method1
```
This range does not include 0, we 


Method 2
 
```{r}
method2 <- c(statistic=dispersiontest(modp)$statistic, p.value=dispersiontest(modp)$p.value)

print("Method 2")
method2

```
The p value is significant. We reject the null hypothesis that c=0, we conclude that it is not a poisson distribution.



Method 3 GLM.NB
```{r}
modn <- glm.nb(skips~., solder)
odTest(modn)
```
We reject the null hypothesis, this signals overdispersion.

Next, we add interaction terms
```{r}
# poisson model with interaction terms
modp2  <- glm(skips ~ (Opening +Solder + Mask + PadType + Panel)^2 , family=poisson, data=solder)
#deviance of the model
deviance(modp2)
```

this fit is better, compaire the AIC measures for both poisson models
```{r}
c(modp=aicComplete, modp2=modp2$aic)
```

```{r}
pchisq(deviance(modp2),df.residual(modp2),lower=FALSE)
```
The above code gives the chi squared distribution function comparing the deviance and the degrees of freedom. Since there is a small probability they are the same, then we say there is overdispersiion

Predict the probability of coun=0, ie "number of solder skips is 0
"
```{r}
coun <- 0
probabil_0 <- dpois(coun, predict(modp2, type="response"))
head(probabil_0)
```

## 3.1.2 Negative binomial regression

Fit negative binomial regression using glm.nb() from MASS
```{r}
modn <- glm.nb(skips ~ ., data=solder)
summary(modn)
```
The parameter Theta/Phi is small, 4.3927

Check if this model is significantly different from Poisson
```{r}
odTest(modn)
```
We see that the p value is small, we reject the null hypothesis, there is overdispersion. This is negative binomail distribution, significantly different from Poisson 

Predict probability of no skips by the negative binomial model
```{r}
coun<-0
probabil_0.nb<-dnbinom(coun,mu=predict(modn,type="response"),size=modn$theta)
head(probabil_0.nb)
```

## 3.2 Homicide victims

The data are from a survey of 1308 people in which they were asked how many homicide victims they know. The variables are:

resp, the number of victims the respondent knows;
race, the race of the respondent (black or white).
Does race help explain how many homicide victims a person knows?

3.2.1 Data
```{r}
black <- c(119,16,12,7,3,2,0)
white <- c(1070,60,14,4,0,0,1)
resp <- c(rep(0:6,times=black), rep(0:6,times=white))
race <- factor(c(rep("black", sum(black)), rep("white", sum(white))),
                levels = c("white","black"))
victim <- data.frame(resp, race)
head(victim)
```

Majority of respondents are white:
```{r}
table(race)
```
Mean counts by race show that mean response from African Americans is higher
```{r}
(countMeans <- with(victim, tapply(resp, race, mean)))
```
Variance of responses from both are higher than the mans, which is a sign of overdispersion
```{r}
(countVariances <- with(victim, tapply(resp, race, var)))
```
Look at the distribution of the counts by race
```{r}
table(resp, race)
```

### 3.2.2 Poisson regression

Fit Poisson regression explaining response by race

```{r}
mPo <- glm(resp ~ race, data=victim, family=poisson)
summary(mPo)
```
Race predictor is significant. Intensity of known homicide victims is significantly higher for black responders(exp1.73 gives the change of intensity). Intensity of known homicide victims for white responders is represented by the intercept (-2.38)

Deviance to df looks good
```{r}
# exp intercept
exp(coef(mPo)[1])

```
Intensity for black responders is more than 5 times higher
```{r}
exp(coef(mPo)[1])*exp(coef(mPo)[2])
exp(coef(mPo)[1]+coef(mPo)[2])
```
Compare sample and model means
```{r}
rbind(Sample=countMeans,Model=cumprod(exp(coef(mPo))))
```
and compare sample and model variances
```{r}
rbind(Sample=countVariances,Model=cumprod(exp(coef(mPo))))
```
Obviously, means are estimated exactly, but variances did not estimate very well. This means that something is wrong with the model, we can't capture the variances of the samples

Visualize the fit by Poisson regression using rootogram() from countreg

```{r}
library(countreg)
countreg::rootogram(mPo)
```

The red line shows square root of fitted Poisson frequency.
"Hanging" from each point on the red line is a bar, the height of which represents the residuals: difference between expected and observed counts.
A bar hanging below 0 indicates underfitting, where the observed frequency is greater than the predicted frequency. A bar hanging above 0 indicates overfitting, where the observed frequency is less than the predicted frequency.
The counts are transformed with a square root transformation to prevent smaller counts from getting obscured and overwhelmed by larger counts.

Note underfitting for counts 2 and higher and overfitting for the 1 count.

Predicted probabilities of response equal from 0 to 6 by both races.
```{r}
predBlack<-sapply(0:6,function(z) 
  dpois(z,predict(mPo,newdata=data.frame(race=c("black")),type="response")))

predWhite<-sapply(0:6,function(z) 
  dpois(z,predict(mPo,newdata=data.frame(race=c("white")),type="response")))

plot(0:6,predWhite,type="b")
points(0:6,predBlack,col="red",type="b")
```

### 3.2.3 Negative binomial regression

Fit negative binomial model using glm.nb() from MASS
```{r}
mNb <- glm.nb(resp ~ race, data=victim)
summary(mNb)
```

Deviance seems good. Theta/phi is small, tells us 


Even though poisson fit looks fine, we can predict the variances better

and the predictions are much closer
```{r}
Mus<-sort(unique(predict(mNb,type="response")))
rbind(Sample=countMeans,Model=Mus)
rbind(Sample=countVariances,Model=Mus+Mus^2/mNb$theta)
countreg::rootogram(mNb)
```
Plot the predicted probabilities
```{r}
mu<-predict(mPo,newdata=data.frame(race=c("black")),type="response")
predBlack<-sapply(0:6,function(z) dnbinom(z,mu=mu,size=mNb$theta))
mu<-predict(mPo,newdata=data.frame(race=c("white")),type="response")
predWhite<-sapply(0:6,function(z) dnbinom(z,mu=mu,size=mNb$theta))

plot(0:6,predWhite,type="b")
points(0:6,predBlack,col="red",type="b")
legend("topright",legend=c("white","black"),col=c("black","red"),pch=1)
```

## 3.3 Demand for medical care by elderly

### 3.3.1 data

Deb and Trivedi data DebTrivedi.rda and the description of this project can be downloaded from here.

The data contain records on 4406 individuals, aged 66 and over, who are covered by Medicare, a public insurance program.

Originally the data were obtained from the US National Medical Expenditure Survey (NMES) for 1987/88.

The objective is to model the demand for medical care as captured by the number of physician/non-physician office and hospital outpatient visits by the covariates available for the patients.

```{r}
load(paste(dataPath,"DebTrivedi.rda",sep="/"))
dat <- DebTrivedi[, c(1, 6:8, 13, 15, 18)]
head(dat)
```

```{r}
dat$health<-as.factor(dat$health)
dat$gender<-as.factor(dat$gender)
dat$privins<-as.factor(dat$privins)
```

Use the number of physician office visits ofp as the dependent variable and as regressors use:

The health status variables:
hosp (number of hospital stays),
health (self-perceived health status),
numchron (number of chronic conditions),
And socioeconomic variables, such as:
gender,
school (number of years of education),
privins (private insurance indicator).
Observe the histogram of the response.

```{r}
# plot the number of counts of office visits, alot of 0s
plot(table(dat$ofp))
```

The plot shows significant number of zeros and high variance.

Prepare functions for visualization of the relationships between each predictor and the response as here.

```{r}
clog <- function(x) log(x + 0.5) # Continuity corrected log
cfac <- function(x, breaks = NULL) { # make count variable a factor
 if(is.null(breaks)) breaks <- unique(quantile(x, 0:10/10))
 x <- cut(x, breaks, include.lowest = TRUE, right = FALSE)
 levels(x) <- paste(breaks[-length(breaks)], ifelse(diff(breaks) > 1,
 c(paste("-", breaks[-c(1, length(breaks))] - 1, sep = ""), "+"), ""),
 sep = "")
 return(x)
}
```
Plot the pairwise reationships
```{r}
plot(clog(ofp) ~ cfac(numchron), data = dat)
```
Here we can see that the number of visits increases with the number of chronic conditions
```{r}
plot(clog(ofp) ~ health, data = dat, varwidth = TRUE)
```
The number of visits decreases with better health
```{r}
plot(clog(ofp) ~ privins, data = dat, varwidth = TRUE)
```

The patientes with private insurance make more visits, but have fat tails on the upside
```{r}
plot(clog(ofp) ~ cfac(hosp, c(0:2, 8)), data = dat)
```
The number of hosptal stays is positively correlated with the number of visits to doctors
```{r}
plot(clog(ofp) ~ gender, data = dat, varwidth = TRUE)
```
Medians and upper quartiles are very similar between men and women. Bu men have lower 25% quartile, higher non-outlier maximum and longer tail of outliers on the upside.

```{r}
plot(cfac(ofp, c(0:2, 4, 6, 10, 100)) ~ school, data = dat, breaks = 9)
```
With number of years of education the number of visits grows

### 3.3.2 Poisson regression

```{r}
fm_pois <- glm(ofp ~ ., data = dat, family = poisson)
summary(fm_pois)
```
All Poisson coefficients are significant. Bad deviance, probably underfitting

Interpret signs and values of the coefficients

```{r}
exp(coef(fm_pois))
```
test for overdispesion
```{r}
dispersiontest(fm_pois)
```
We reject the null hypothesis. This is not poisson, there is overdispersion

```{r}
countreg::rootogram(fm_pois)
```

Rootogram shows poor fit. Model struggles to fit both 0s and 1s

### 3.3.3 Quasi- poisson regression

One way of dealing with over-dispersion is to use the mean regression function and the variance function from the Poisson GLM but to leave the dispersion parameter unrestricted.

Thus, dispersion is not assumed to be fixed at 1 but is estimated from the data.
This leads to the same coefficient estimates as the standard Poisson model but inference is adjusted for over-dispersion.
If estimated dispersion parameter is greater than 1 we conclude that there is overdispersion.

In R, the quasi-Poisson model with estimated dispersion parameter can also be fitted with the glm() function, simply setting family = quasipoisson.

```{r}
fm_qpois <- glm(ofp ~ ., data = dat, family = quasipoisson)
summary(fm_qpois)
```

```{r}
summary(fm_qpois)$dispersion
```
Which is a large number in comparison with 1: quasi-Poisson model was able to capture some overdispersion.
Deviance did not improve alot, we did capture some overdispersion

###3.3.4 negative binomail regression


```{r}
fm_nbin <- glm.nb(ofp ~ ., data = dat)
summary(fm_nbin)
```
Regression coefficients and standard errors of fm_nbin and fm_qpois are similar.
We expect similar predictions and conclusions from both of them.

Negative binomial model is more theoretically solid and is based on formal likelihood function without any adjustments.
From Negative binomial model we can calculate directly probability of number of zeros.
```{r}
countreg::rootogram(fm_nbin)
```
Negative binomial rootogram looks better than Poisson.

Calculate probability of zero count by negative binomial regression and compare it with the observed frequency.

```{r}
coun<-0
mu<-predict(fm_nbin,type="response")
probabil_0.nb<-dnbinom(coun,mu=mu,size=fm_nbin$theta)
head(probabil_0.nb)
```


### 3.3.5 Hurdle and zero inflated models

Often overdispersion is a result of increased number of zeros in the sample.
Two following types of models are known to account for that.

#### 3.3.5.1 Hurdle model

Hurdle model will model the data as combination of two models:

1 Model producing only zeros;
2 And model producing only positive counts, so called zero-truncated model.
Hurdle model can be based on Poisson or on negative binomial distribution.

For the zero model part it is common to use logit link, but any binomial link is possible.

To fit hurdle model use function hurdle() from library pscl.

Fit hurdle model with negative binomial family and same parameter sets for zero part of the model and positive part of the model.
```{r}
fm_hurdle0 <- hurdle(ofp ~ ., data = dat, dist = "negbin")
summary(fm_hurdle0)
```
Coefficients are similar to previous models. The zero sub-model does not need health assessment predictor. Log-likelihood has improved noticeably.
```{r}
countreg::rootogram(fm_hurdle0)
```
Note the effect of zero component of the model.

Same model with different sets for count and for zero sub-models:
```{r}
fm_hurdle <- hurdle(ofp ~ . | hosp + numchron + privins + school + gender,
                    data = dat, dist = "negbin")
summary(fm_hurdle)
```
```{r}
countreg::rootogram(fm_hurdle)
```

#### 3.3.5.2 Zero-inflated model

Zero-inflated model is a mix of a zero-producing model and regular Poisson or negative binomial model.
The zero-making part is logistic.

Equation of zero-inflated Poisson model (ZIP) is
P(yi=0)=pi+(1???pi)e?????i,
P(yi=k)=(1???pi)e?????i??kik!,
or
??i=pi×0+(1???pi)e??0+??1x1+..

In general, for Poisson or negative binomial model
P(yi=0)=pi+(1???pi)P(C=0),
P(yi=k)=(1???pi)P(C=k),
where P(C=k) is probability of count C equal to k given by the count component model.

Function for fitting zero-inflated model iszeroinf(), library pscl.

Fit zero-inflated negative binomial model with identical complete sets of predictors for both sub-models and with different sets.

```{r}
fm_zinb0 <- zeroinfl(ofp ~ ., data = dat, dist = "negbin")
summary(fm_zinb0)
```

